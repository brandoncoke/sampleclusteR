"parent",
"free.medi",
"untransfected",
"ntc",
"mirNC")
label_score= rep(0, length(sample_names))
label_score[grepl(paste(up_reg_labels, collapse = "|"), sample_names,
ignore.case = T)]= 1
label_score[grepl(paste(down_reg_labels, collapse = "|"), sample_names,
ignore.case = T)]= 2
label_score[grepl(paste(control_labels, collapse = "|"), sample_names,
ignore.case = T)]= 0
return(label_score)
}
sample_names= c("WT", "WT", "miNC", "niNC", "KD_COL1A1", "KD_COL1A1", "COL1A1_induced", "COL1A1_+")
get_score_labels2(sample_names)
#based on these sample cluster names
sample_names
sample_names
get_score_labels("kd")
sample_names= c("WT", "WT", "miNC", "niNC", "KD_COL1A1", "KD_COL1A1", "COL1A1_induced", "COL1A1_+")
get_score_labels2(sample_names)
#based on these sample cluster names
sample_names
lapply(sample_names, get_score_labels)
as.numeric(lapply(sample_names, get_score_labels))
get_score_labels= function(sample_names){
up_reg_labels = c("overexp", "express", "transgen", "expos", "tg", "induc",
"stim", "treated", "transfected", "overexpression",
"transformed", "tumor", "tomour"
)
down_reg_labels <- c("knock", "null",
"s[hi]rna",
"delet",
"si[a-zA-z]",
"reduc",
"kd",
"\\-\\/",
"\\/\\-",
"\\+\\/", "\\/\\+",
"cre", "flox",
"mut",
"defici",
"[_| ]ko[_| ]|[_| ]ko$")
control_labels <- c("untreat", "_ns_",
"normal",
"^wt$|^wt[_| ]",
"gfp",
"vehicle",
"sensitive",
"stable",
"ctrl",
"non.sense",
"nonsense",
"baseline",
"mock",
"_luc_|_luc",
"siluc",
"wildtype|wild.type",
"nontreat",
"non.treated",
"control",
"ctrl",
"untreated",
"no.treat",
"undosed",
"untransfected",
"mir.nc",
"minc",
"_non_|^non_",
"scramble",
"lucif",
"parent",
"free.medi",
"untransfected",
"ntc",
"mirNC")
label_score= rep(0, length(sample_names))
label_score[grepl(paste(up_reg_labels, collapse = "|"), sample_names,
ignore.case = T)]= 1
label_score[grepl(paste(down_reg_labels, collapse = "|"), sample_names,
ignore.case = T)]= 2
label_score[grepl(paste(control_labels, collapse = "|"), sample_names,
ignore.case = T)]= 0
return(label_score)
}
remove.packages("sampleclusteR")
library(sampleclusteR)
sample_names= c("WT", "WT", "miNC", "niNC", "KD_COL1A1", "KD_COL1A1", "COL1A1_induced", "COL1A1_+")
get_score_labels(sample_names)
#based on these sample cluster names
sample_names
remove.packages("sampleclusteR")
library(sampleclusteR)
remove.packages("sampleclusteR")
library(sampleclusteR)
gset= obtain.gset("GSE84881")
gsm_titles= obtain.gsm.titles(gset)
title_clusters= title.clustering(gsm_titles)
groups= as.integer(lapply(gsm_titles, assign.group, title_clusters))
exp_groups= set.exp.groups(groups)
gsms= exp_groups
gsms= paste(gsms, collapse= "")
Biobase::fvarLabels(GEOset) <- make.names(fvarLabels(GEOset))
exp_groups= exp_groups[1]
exp_groups
exp_groups= unlist(exp_groups)
gsms= exp_groups
gsms= paste(gsms, collapse= "")
Biobase::fvarLabels(GEOset) <- make.names(fvarLabels(GEOset))
ex <- GEOset@assayData[["exprs"]] #Dataframe of expression
GEOset
GEOset= gset
gsms= exp_groups
gsms= paste(gsms, collapse= "")
Biobase::fvarLabels(GEOset) <- make.names(fvarLabels(GEOset))
sml <- c() #enables sml to be called as a function for loop below.
for (i in 1:nchar(gsms)) { sml[i] <- substr(gsms,i,i) } #Adding labels to microarray data.
sel <- which(sml != "X")
sml <- sml[sel]
#if(length(sml[sml == "0"]) > 1 & length(sml[sml == "1"]) > 1){ #old function exit- automated.analysis already gets rid of experiment sets with a single replicate
#  stop("Too few replicates in this cluster- manual intervention required!")
#}
ex <- GEOset@assayData[["exprs"]] #Dataframe of expression
qx <- as.numeric(quantile(ex, c(0., 0.25, 0.5, 0.75, 0.99, 1.0), na.rm=T)) #Getting quartiles for values
LogC <- (qx[5] > 100) ||
(qx[6]-qx[1] > 50 && qx[2] > 0) ||
(qx[2] > 0 && qx[2] < 1 && qx[4] > 1 && qx[4] < 2)
if(is.na(LogC)){LogC = F}# bodge if not much
if (LogC) { ex[which(ex <= 0)] <- NaN #Gets rid of extreme outliers
Biobase::exprs(GEOset) <- log2(ex) }
gs <- factor(sml)
groups <- make.names(c("1","2"))
levels(gs) <- groups
GEOset$group <- gs
design <- model.matrix(~group + 0, GEOset)
colnames(design) <- levels(gs)
fit <- limma::lmFit(GEOset, design)
GEOset
design
cont.matrix
subset_count_table
design
groups
cts
cts <- paste(groups[1], groups[2], sep="-")
cts
ff <- log(Volume) ~ log(Height) + log(Girth)
utils::str(m <- model.frame(ff, trees))
mat <- model.matrix(ff, m)
mat
ff
GEO_id="GSE39097"; path="~/"; meta_data_and_combined=F; platform= "NONE"; limma_or_rankprod= "limma"; words_only= F
gset= obtain.gset(GEO_id, platform= platform) #getting expression dataset
require(sampleclusteR)
gset= obtain.gset(GEO_id, platform= platform) #getting expression dataset
if(length(gset@phenoData@data[["geo_accession"]]) < 2 ){ #quick exit if only one or 2 samples present- limma reqs 2 reps in at least 2 groups
stop("Too few samples for limma!")
}
if(is.null(gset) | class(gset) != "ExpressionSet"){
stop("gset NOT VALID")
}
combined_table= get.combined.table(gset,T)
combined_table
gsm_titles= gset@phenoData@data$title #origianlly due to get.cluster.titles reusing names of variables- avoid at all costs
title_clusters= title.clustering(gsm_titles, words_only= words_only)
#if clustering did not work
if(length(title_clusters) == nrow(combined_table)){
message("Study is likely to have patient/sample ids. Removing them during clustering")
title_clusters= title.clustering(gsm_titles, words_only= T)
}
#too many clusters- might be due to junk patient ids
if(length(title_clusters) > nrow(combined_table)*.75 &
nrow(combined_table) > 6){
message("Study is likely to have patient/sample ids. Removing them during clustering")
title_clusters= title.clustering(gsm_titles, words_only= T)
}
#instances where titles are all unique- so cannot cluster
if(length(title_clusters) > (nrow(combined_table)-1) |
as.character(title_clusters[1]) == "character(0)"){
combined_table$titleTable= "_"
gsm_titles= rep("Sample", nrow(combined_table))
title_clusters= list("Sample")
}
combined_table$title_groups= as.integer(lapply(gsm_titles, assign.group,
title_clusters))
combined_table
#Characteristic clustering
gsm_characteristics= obtain.charateristics(gset)
charateristics_clusters= characteristic.clustering(gsm_characteristics)
#instances where characteristics are all unique- so cannot cluster
if(length(charateristics_clusters) > (nrow(combined_table)-1)){
combined_table$characteristicsTable= "_"
gsm_characteristics= rep("Sample", nrow(combined_table))
charateristics_clusters= list("Sample")
}
combined_table$characteristic_groups= as.integer(lapply(gsm_characteristics,
assign.group,
charateristics_clusters))
#Source clustering
gsm_source= obtain.gsm.source(gset)
source_clusters= source.clustering(gsm_source)
#instances where source are all unique- so cannot cluster
if(length(source_clusters) > (nrow(combined_table)-1)){
combined_table$souceInfo= "_"
gsm_source= rep("Sample")
source_clusters= list("Sample")
}
combined_table$soruce_groups= as.integer(lapply(gsm_source,
assign.group,
source_clusters))
#Combined clustering
#combined groups
comb_groups= apply(combined_table[,4:6], 1, paste, collapse= "")
unique_combs= unique(comb_groups)
comb_groups= as.integer(lapply(comb_groups, function(x){
which(x == unique_combs)
}))
combined_table$combined_groups= comb_groups
#remove excess text from combined text
unique_features= which(as.logical(apply(combined_table[,1:3], 2,
function(x){
length(unique(x)) > 1
})))
combined_table
switch(length(unique_features),
unique_features= unique_features,
unique_features= unique.feature.redun(unique_features, combined_table),
unique_features= unique.feature.redun(unique_features, combined_table),
unique_features= unique.feature.redun(unique_features, combined_table) #why?
)
unique_features
if(length(unique_features)> 1){
comb_titles= as.character(apply(combined_table[,unique_features], 1, paste, collapse= " "))
}else{
comb_titles= combined_table[,unique_features]
}
message(paste("Analysing", GEO_id)) #user aware of dataset analysed
gset= obtain.gset(GEO_id, platform= platform) #getting expression dataset
if(length(gset@phenoData@data[["geo_accession"]]) < 2 ){ #quick exit if only one or 2 samples present- limma reqs 2 reps in at least 2 groups
stop("Too few samples for limma!")
}
if(is.null(gset) | class(gset) != "ExpressionSet"){
stop("gset NOT VALID")
}
combined_table= get.combined.table(gset,T)
combined_table
gsm_titles= gset@phenoData@data$title #origianlly due to get.cluster.titles reusing names of variables- avoid at all costs
title_clusters= title.clustering(gsm_titles, words_only= words_only)
#if clustering did not work
if(length(title_clusters) == nrow(combined_table)){
message("Study is likely to have patient/sample ids. Removing them during clustering")
title_clusters= title.clustering(gsm_titles, words_only= T)
}
#too many clusters- might be due to junk patient ids
if(length(title_clusters) > nrow(combined_table)*.75 &
nrow(combined_table) > 6){
message("Study is likely to have patient/sample ids. Removing them during clustering")
title_clusters= title.clustering(gsm_titles, words_only= T)
}
#instances where titles are all unique- so cannot cluster
if(length(title_clusters) > (nrow(combined_table)-1) |
as.character(title_clusters[1]) == "character(0)"){
combined_table$titleTable= "_"
gsm_titles= rep("Sample", nrow(combined_table))
title_clusters= list("Sample")
}
combined_table$title_groups= as.integer(lapply(gsm_titles, assign.group,
title_clusters))
#if the titles
#Characteristic clustering
gsm_characteristics= obtain.charateristics(gset)
charateristics_clusters= characteristic.clustering(gsm_characteristics)
#instances where characteristics are all unique- so cannot cluster
if(length(charateristics_clusters) > (nrow(combined_table)-1)){
combined_table$characteristicsTable= "_"
gsm_characteristics= rep("Sample", nrow(combined_table))
charateristics_clusters= list("Sample")
}
combined_table$characteristic_groups= as.integer(lapply(gsm_characteristics,
assign.group,
charateristics_clusters))
#Source clustering
gsm_source= obtain.gsm.source(gset)
source_clusters= source.clustering(gsm_source)
#instances where source are all unique- so cannot cluster
if(length(source_clusters) > (nrow(combined_table)-1)){
combined_table$souceInfo= "_"
gsm_source= rep("Sample")
source_clusters= list("Sample")
}
combined_table$soruce_groups= as.integer(lapply(gsm_source,
assign.group,
source_clusters))
#Combined clustering
#combined groups
comb_groups= apply(combined_table[,4:6], 1, paste, collapse= "")
unique_combs= unique(comb_groups)
comb_groups= as.integer(lapply(comb_groups, function(x){
which(x == unique_combs)
}))
combined_table$combined_groups= comb_groups
#remove excess text from combined text
unique_features= which(as.logical(apply(combined_table[,1:3], 2,
function(x){
length(unique(x)) > 1
})))
switch(length(unique_features),
unique_features= unique_features,
unique_features= unique.feature.redun(unique_features, combined_table),
unique_features= unique.feature.redun(unique_features, combined_table),
unique_features= unique.feature.redun(unique_features, combined_table) #why?
)
length(unique_features)
unique_features
unique_features= which(as.logical(apply(combined_table[,1:3], 2,
function(x){
length(unique(x)) > 1
})))
length(unique_features)
if(length(unique_features)> 1){
comb_titles= as.character(apply(combined_table[,unique_features], 1, paste, collapse= " "))
}else{
comb_titles= combined_table[,unique_features]
}
unique_features == 0
unique_features= which(as.logical(apply(combined_table[,1:3], 2,
function(x){
length(unique(x)) > 1
})))
unique_features
as.integer(unique_features)
as.integer(unique_features) == 0
(unique_features == 0)
as.logical(unique_features == 0)
!(unique_features == 0)
which(as.logical(apply(combined_table[,1:3], 2,
function(x){
length(unique(x)) > 1
})))
as.logical(apply(combined_table[,1:3], 2,
function(x){
length(unique(x)) > 1
}))
any(as.logical(apply(combined_table[,1:3], 2,
function(x){
length(unique(x)) > 1
})))
!any(as.logical(apply(combined_table[,1:3], 2,
function(x){
length(unique(x)) > 1
})))
"a sentence  with     space__errors"
b= "a sentence  with     space__errors"
b
title= b
while(grepl("  |__", title)){
gsub("  |__", title)
}
while(grepl("  |__", title)){
title= gsub("  |__", "", title)
}
title= b
while(grepl("  |__", title)){
title= gsub("  |__", "_", title)
}
title
#get rid of any empty spcaces
while(grepl("  |__|_ ", title)){
title= gsub("  |__", "_", title)
}
title= b
while(grepl("  |__|_ ", title)){
title= gsub("  |__|_ ", "_", title)
}
title
title= b
while(grepl("  |__|_ | ", title)){
title= gsub("  |__|_ | ", "_", title)
}
title
#yt-dlp ytsearch1:"rip bozo" --get-id --get-title
################################################################################
#Package install
################################################################################
if (!requireNamespace("stringr", quietly = TRUE))
install.packages("stringr")
require(stringr)
if (!requireNamespace("RSQLite", quietly = TRUE))
install.packages("RSQLite")
#if (!requireNamespace("xml2", quietly = TRUE))
#  install.packages("xml2")
#library(xml2)
#if (!requireNamespace("XML", quietly = TRUE))
#  install.packages("XML")
#library(XML)
################################################################################
#Page dowload and video extract
################################################################################
setwd("~")
channels= read.csv("~/yt_offline/youtube_csv.csv")
channels= channels[ , 1]
# check if domain can be reached via port 80 ignore- whats a trycatch
#is_up(destination = "example.com")
get_xml= function(
channel_link= "https://www.youtube.com/feeds/videos.xml?channel_id=UCP5tjEmvPItGyLhmjdwP7Ww"){
out <- tryCatch(
{message(paste0("Obtaining XML"))
return(readLines(channel_link))
},
error=function(cond) { #basically a catch if aint getting anything
print("Link doesn't exist")
print(channel_link)
return(NULL)
}
)
}
#can'parse the XML- eh regex the info out
get_channel_videos= function(raw_html){
#raw_raw_html= paste(raw_raw_html, collapse = "")
#xml= read_xml(channel_link) #you really think im going to do it properly
if(is.null(raw_html)){
return(data.frame(titles= NA, links= NA, publish_times=NA, channel_name= NA))
}else{
title_indices= grep("<media:title>", raw_html)
titles= as.character(
lapply(title_indices,
extract_xml_info,
pattern= "<media:title>")
)
links_incides= grep("<yt:videoId>", raw_html)
links= as.character(
lapply(links_incides,
extract_xml_info,
pattern= "<yt:videoId>")
)
links= gsub(" ", "", links)
links= paste0(
"https://www.youtube.com/watch?v=",
links
)
publish_time_indices= grep("<published>", raw_html)
publish_times= as.character(
lapply(publish_time_indices,
extract_xml_info,
pattern= "<published>")
)
publish_times= gsub(" ", "", publish_times)
publish_times= get_times(publish_times)
publish_times= publish_times[-1] #first is the channels inception
channel_name= grep("<name>", raw_html)
channel_name= extract_xml_info(channel_name[1], pattern= "<name>")
while(substr(channel_name, 1, 1) == " "){
channel_name= gsub("^ ", "", channel_name)
}
#produce dataframe from channel
data.frame(titles, links, publish_times, channel_name)
}
}
extract_xml_info= function(xml_indice= 26, pattern= "<media:title>"){
regex_for_gsub= paste0(pattern,"|<[/]",substr(pattern, 2,nchar(pattern)))
useful_info= gsub(regex_for_gsub, "", html[xml_indice])
gsub("^   ", "",useful_info)
}
get_times= function(publish_time= publish_times[1]){
date= substr(publish_time, 1, str_locate(publish_time, "T")[1] - 1)
time= substr(publish_time, str_locate(publish_time, "T")[1] + 1,
str_locate(publish_time, "[+]")[1] - 1)
as.POSIXct(paste(as.character(date),
as.character(time)),
format= "%Y-%m-%d %H:%M:%OS",
tz = "GMT")
}
html= get_xml(channels[1])
channel_videos= get_channel_videos(html)
#have for loop- readLines screwing me over
for(i in 2:length(channels)){
html= get_xml(channels[i])
temp= get_channel_videos(html)
channel_videos= rbind(channel_videos, temp)
}
channel_videos= channel_videos[order(channel_videos$publish_times, decreasing = T), ]
channel_videos= channel_videos[!is.na(channel_videos$links), ]
#done!
write.csv(channel_videos, "~/yt_offline/channel_videos.csv", row.names = F)
#.mode csv and  .import channel_videos.csv feed
if (!requireNamespace("RSQLite", quietly = TRUE))
install.packages("RSQLite")
require(RSQLite)
sqlite_location= "~/yt_offline/yt_feed.sqlite3"
mydb= dbConnect(SQLite(),sqlite_location)
dbListTables(mydb)
RSQLite::dbWriteTable(mydb, "yt_feed", channel_videos, append= T)
#Get unique entries
RSQLite::dbSendStatement(mydb, "DELETE FROM yt_feed WHERE EXISTS (SELECT 1 FROM yt_feed p2 WHERE yt_feed.links = p2.links AND yt_feed.rowid > p2.rowid)")
#Cleanup
RSQLite::dbSendStatement(mydb, "VACUUM")
#Now get full feed- watch out sqlite converts times to integers
full_yotube_feed= dbGetQuery(mydb, "SELECT * FROM yt_feed ORDER BY publish_times DESC limit 1000")
temp= full_yotube_feed[grepl("[-]", full_yotube_feed$publish_times), ]
temp$publish_times= as.POSIXct(temp$publish_times)
full_yotube_feed= full_yotube_feed[!grepl("[-]", full_yotube_feed$publish_times), ]
full_yotube_feed$publish_times= as.numeric(full_yotube_feed$publish_times)
full_yotube_feed$publish_times= as.POSIXct(full_yotube_feed$publish_times,
origin= "1970-01-01")
full_yotube_feed= rbind(full_yotube_feed, temp)
View(full_yotube_feed[order(full_yotube_feed$publish_times, decreasing = T), ])
library(sampleclusteR)
remove.packages("sampleclusteR")
library(sampleclusteR)
remove.packages("sampleclusteR")
library(sampleclusteR)
